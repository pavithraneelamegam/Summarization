{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29543dad-62a6-45e4-9311-0ac33478f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, json, jsonlines, math, time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    BartTokenizer, BartForConditionalGeneration,\n",
    "    PegasusTokenizer, PegasusForConditionalGeneration,\n",
    "    LEDTokenizer, LEDForConditionalGeneration\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Retriever model (recommend a legal SBERT if available)\n",
    "RETRIEVER_MODEL = \"law-ai/InLegalBERT\"  # replace with legal SBERT for better perf\n",
    "\n",
    "# Generator checkpoints (use your fine-tuned models / local paths)\n",
    "BART_MODEL = \"bart_legal_summ_model_final\"\n",
    "PEGASUS_MODEL = \"pegasus_legal_summ_model_final\"\n",
    "LED_MODEL = \"led_legal_summ_model_final\"\n",
    "\n",
    "# Files\n",
    "TEST_PATH = \"test_judg.jsonl\"   # input JSONL with {\"id\": \"...\", \"judgment\": \"...\"}\n",
    "OUT_SUMMARIES = \"generated_summaries_rag.jsonl\"\n",
    "OUT_COSINE = \"cosine_scores_rag.jsonl\"\n",
    "\n",
    "# Chunking & retrieval params\n",
    "CHUNK_SIZE_TOKENS = 800       # chunk token length when splitting (approx)\n",
    "CHUNK_OVERLAP_TOKENS = 200\n",
    "TOP_K = 8                     # number of chunks to retrieve per judgment (initial)\n",
    "RETRIEVE_BY = \"sentence\"      # \"sentence\" or \"token\" chunking\n",
    "\n",
    "# Generation: enforce ~400-500 words\n",
    "MIN_WORDS = 400\n",
    "MAX_WORDS = 500\n",
    "TOK_PER_WORD = 1.5            # rough token estimate per word => tokens = words * TOK_PER_WORD\n",
    "GEN_MIN_TOKENS = int(MIN_WORDS * TOK_PER_WORD)   # e.g., 400 * 1.5 = 600\n",
    "GEN_MAX_TOKENS = int(MAX_WORDS * TOK_PER_WORD)   # e.g., 500 * 1.5 = 750\n",
    "\n",
    "# Retrieval-to-generation flow\n",
    "MAX_RETRIEVE_ROUNDS = 3       # if initial generation is too short, you can expand retrieval rounds\n",
    "\n",
    "# Safety / diversity thresholds\n",
    "SENTENCE_SIM_DIVERSITY = 0.85\n",
    "\n",
    "# ---------------------------\n",
    "# UTILITIES\n",
    "# ---------------------------\n",
    "def clean_judgment_text(text):\n",
    "    text = re.sub(r\"\\[Page No\\.\\s*\\d+\\]\", \" \", text)\n",
    "    text = re.sub(r\"Case\\s*:-.*?\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\(\\d+\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.replace(\" ,\", \",\").replace(\" .\", \".\").strip()\n",
    "    return text\n",
    "\n",
    "# simple splitter by sentences to produce retrieval units\n",
    "def chunk_into_sentences(text):\n",
    "    sents = sent_tokenize(text)\n",
    "    return [s.strip() for s in sents if len(s.strip())>20]\n",
    "\n",
    "# token-based chunking using tokenizer.encode/decode for better boundaries\n",
    "def chunk_by_tokens(text, tokenizer, max_tokens=CHUNK_SIZE_TOKENS, overlap=CHUNK_OVERLAP_TOKENS):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [tokenizer.decode(tokens, clean_up_tokenization_spaces=True)]\n",
    "    chunks = []\n",
    "    step = max_tokens - overlap\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk_tokens = tokens[i:i+max_tokens]\n",
    "        if not chunk_tokens:\n",
    "            break\n",
    "        chunks.append(tokenizer.decode(chunk_tokens, clean_up_tokenization_spaces=True))\n",
    "        if i + max_tokens >= len(tokens):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "def words_count(text):\n",
    "    return len(re.findall(r\"\\w+\", text))\n",
    "\n",
    "def truncate_to_word_limit_by_sentences(summary, min_words=MIN_WORDS, max_words=MAX_WORDS):\n",
    "    words = summary.split()\n",
    "    if len(words) <= max_words and len(words) >= min_words:\n",
    "        return summary\n",
    "    sents = sent_tokenize(summary)\n",
    "    out = []\n",
    "    w = 0\n",
    "    for s in sents:\n",
    "        sw = len(s.split())\n",
    "        if w + sw > max_words:\n",
    "            break\n",
    "        out.append(s)\n",
    "        w += sw\n",
    "    # if result shorter than min_words, append first sentences until min reached (from original)\n",
    "    if w < min_words:\n",
    "        # append more sentences (allow some exceeding max)\n",
    "        for s in sents[len(out):]:\n",
    "            out.append(s)\n",
    "            w += len(s.split())\n",
    "            if w >= min_words:\n",
    "                break\n",
    "    return \" \".join(out)\n",
    "\n",
    "# ---------------------------\n",
    "# LOAD MODELS\n",
    "# ---------------------------\n",
    "print(\"Loading retriever and generators (may take a while)...\")\n",
    "retriever = SentenceTransformer(RETRIEVER_MODEL, device=device)\n",
    "\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(BART_MODEL)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(BART_MODEL).to(device).eval()\n",
    "\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(PEGASUS_MODEL)\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(PEGASUS_MODEL).to(device).eval()\n",
    "\n",
    "led_tokenizer = LEDTokenizer.from_pretrained(LED_MODEL)\n",
    "led_model = LEDForConditionalGeneration.from_pretrained(LED_MODEL).to(device).eval()\n",
    "\n",
    "# we use BART encoder mean-pooled embeddings for cosine evaluation\n",
    "def bart_encode_mean(text, max_length=1024):\n",
    "    inputs = bart_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        enc = bart_model.model.encoder(**inputs)\n",
    "    return enc.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# generation helper\n",
    "def generate_with(model, tokenizer, input_text, min_tokens=GEN_MIN_TOKENS, max_tokens=GEN_MAX_TOKENS, model_type=\"bart\"):\n",
    "    if model_type == \"led\":\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=LED_MODEL and 16000 or 8192).to(device)\n",
    "    else:\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "\n",
    "    out_ids = model.generate(\n",
    "        **inputs,\n",
    "        num_beams=4,\n",
    "        max_length=max_tokens,\n",
    "        min_length=min_tokens,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(out_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# ---------------------------\n",
    "# RAG Workflow per judgment\n",
    "# ---------------------------\n",
    "def build_chunk_corpus(text, method=\"token\"):\n",
    "    \"\"\"Return list of chunks (strings) for this judgment\"\"\"\n",
    "    if method == \"sentence\":\n",
    "        # chunk by grouping N sentences per chunk to reach approx chunk size\n",
    "        sents = chunk_into_sentences(text)\n",
    "        chunks = []\n",
    "        N = 8  # start grouping ~8 sentences (tunable)\n",
    "        for i in range(0, len(sents), N):\n",
    "            chunks.append(\" \".join(sents[i:i+N]))\n",
    "        return chunks\n",
    "    else:\n",
    "        # token-based chunking using LED tokenizer for stable tokenization\n",
    "        return chunk_by_tokens(text, led_tokenizer, max_tokens=CHUNK_SIZE_TOKENS, overlap=CHUNK_OVERLAP_TOKENS)\n",
    "\n",
    "def retrieve_top_k(chunks, query_embedding, k=TOP_K):\n",
    "    # compute embeddings for chunks and cosine similarity\n",
    "    chunk_embs = retriever.encode(chunks, convert_to_tensor=True, show_progress_bar=False)\n",
    "    scores = util.cos_sim(query_embedding, chunk_embs)[0]  # tensor\n",
    "    topk = torch.topk(scores, min(k, len(chunks)))[1].cpu().numpy().tolist()\n",
    "    # return (idx, chunk_text, score)\n",
    "    return [(i, chunks[i], float(scores[i].cpu().item())) for i in topk]\n",
    "\n",
    "def rag_summarize_judgment(judgment_text, rounds=1):\n",
    "    \"\"\"Main RAG summarization for single judgment. rounds: allow expanding retrieval if needed.\"\"\"\n",
    "    # Prepare chunks and their embeddings\n",
    "    chunks = build_chunk_corpus(judgment_text, method=RETRIEVE_BY)\n",
    "    if len(chunks) == 0:\n",
    "        return \"\", 0.0\n",
    "\n",
    "    # Use the whole judgment as query (or you can craft a query)\n",
    "    query_emb = retriever.encode(judgment_text, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "    # initial retrieval\n",
    "    current_k = TOP_K\n",
    "    retrieved = retrieve_top_k(chunks, query_emb, k=current_k)\n",
    "    retrieved_text = \" \".join([r[1] for r in retrieved])\n",
    "\n",
    "    # create generator input (prepend instruction prompt)\n",
    "    prompt = \"Summarize the following legal judgment excerpts into a coherent abstractive summary (400-500 words):\\n\\n\"\n",
    "    gen_input = prompt + retrieved_text\n",
    "\n",
    "    # Primary generation with BART\n",
    "    summary = generate_with(bart_model, bart_tokenizer, gen_input, model_type=\"bart\")\n",
    "\n",
    "    # If length constraints not satisfied, try expand retrieval and/or use LED to meta-summarize\n",
    "    words = words_count(summary)\n",
    "    round_idx = 1\n",
    "    while (words < MIN_WORDS or words > MAX_WORDS) and round_idx <= rounds:\n",
    "        # expand retrieval: increase k\n",
    "        current_k = min(len(chunks), current_k + TOP_K)\n",
    "        retrieved = retrieve_top_k(chunks, query_emb, k=current_k)\n",
    "        retrieved_text = \" \".join([r[1] for r in retrieved])\n",
    "        gen_input = prompt + retrieved_text\n",
    "\n",
    "        # try LED (better for longer contexts) to produce meta-summary\n",
    "        try:\n",
    "            summary = generate_with(led_model, led_tokenizer, gen_input, model_type=\"led\")\n",
    "        except Exception:\n",
    "            # fallback: re-generate with BART but with new context\n",
    "            summary = generate_with(bart_model, bart_tokenizer, gen_input, model_type=\"bart\")\n",
    "\n",
    "        words = words_count(summary)\n",
    "        round_idx += 1\n",
    "\n",
    "    # Postprocess: enforce 400-500 words using sentence trimming/extension\n",
    "    final_summary = truncate_to_word_limit_by_sentences(summary, min_words=MIN_WORDS, max_words=MAX_WORDS)\n",
    "\n",
    "    # compute cosine sim between judgment and final_summary using BART encoder\n",
    "    j_emb = bart_encode_mean(judgment_text)\n",
    "    s_emb = bart_encode_mean(final_summary)\n",
    "    cos_sim = float(cosine_similarity(j_emb, s_emb)[0][0])\n",
    "\n",
    "    return final_summary, cos_sim\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # load inputs\n",
    "    inputs = []\n",
    "    with jsonlines.open(TEST_PATH) as reader:\n",
    "        for obj in reader:\n",
    "            if \"id\" not in obj or \"judgment\" not in obj:\n",
    "                raise ValueError(\"Input JSONL must contain 'id' and 'judgment' fields.\")\n",
    "            inputs.append({\"id\": obj[\"id\"], \"text\": clean_judgment_text(obj[\"judgment\"])})\n",
    "\n",
    "    print(f\"Loaded {len(inputs)} items\")\n",
    "\n",
    "    writer_sum = jsonlines.open(OUT_SUMMARIES, mode=\"w\")\n",
    "    writer_cos = jsonlines.open(OUT_COSINE, mode=\"w\")\n",
    "\n",
    "    for ex in tqdm(inputs, desc=\"RAG summarization\"):\n",
    "        jid = ex[\"id\"]\n",
    "        text = ex[\"text\"]\n",
    "        try:\n",
    "            summary, cos_sim = rag_summarize_judgment(text, rounds=MAX_RETRIEVE_ROUNDS)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {jid}: {e}\")\n",
    "            summary, cos_sim = \"\", 0.0\n",
    "\n",
    "        writer_sum.write({\"ID\": jid, \"Summary\": summary})\n",
    "        writer_cos.write({\"ID\": jid, \"Cosine_Similarity\": round(cos_sim, 6)})\n",
    "\n",
    "        # small debug print for first few\n",
    "        # if needed, print sample preview\n",
    "        # if idx < 3:\n",
    "        #     print(jid, \"->\", summary[:200], \"...\")\n",
    "    writer_sum.close()\n",
    "    writer_cos.close()\n",
    "    print(\"Done â€” outputs:\")\n",
    "    print(\" \", OUT_SUMMARIES)\n",
    "    print(\" \", OUT_COSINE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
