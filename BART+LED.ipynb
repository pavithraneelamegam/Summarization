{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86285dc4-ad6e-4326-911e-027a48ffc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Legal Ensemble Summarizer â€” BART + LED + InLegalBERT\n",
    "#  Generates summaries (400â€“500 words) for validation set,\n",
    "#    reranks using InLegalBERT, and calculates evaluation scores\n",
    "# =========================================================\n",
    "\n",
    "import torch, re, os, json, jsonlines, shutil\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BartTokenizer, BartForConditionalGeneration,\n",
    "    LEDTokenizer, LEDForConditionalGeneration,\n",
    "    AutoTokenizer, AutoModel\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from sacrebleu.metrics import BLEU\n",
    "import evaluate\n",
    "\n",
    "# =========================================================\n",
    "# 1 Setup\n",
    "# =========================================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# =========================================================\n",
    "# 2 Preprocessing\n",
    "# =========================================================\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'Page\\s*\\d+\\s*of\\s*\\d+', ' ', text)\n",
    "    text = re.sub(r'(Case\\s*No\\.?|Crl\\.A\\.No\\.?|Appeal\\s*No\\.?)\\s*[\\w/-]+', ' ', text)\n",
    "    text = re.sub(r'\\(\\d{4}\\)\\s*\\d+\\s*[A-Z]+\\s*\\d+', ' ', text)\n",
    "    text = re.sub(r'AIR\\s*\\d{4}\\s*[A-Z]+\\s*\\d+', ' ', text)\n",
    "    text = re.sub(r'Dated\\s*[:\\-]?\\s*\\d{1,2}[-./]\\d{1,2}[-./]\\d{2,4}', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# =========================================================\n",
    "# 3 Ensemble Summarizer Class\n",
    "# =========================================================\n",
    "class LegalEnsembleSummarizer:\n",
    "    def __init__(self, bart_path, led_path, reranker_name=\"law-ai/InLegalBERT\"):\n",
    "        # Load BART model\n",
    "        self.bart_tokenizer = BartTokenizer.from_pretrained(bart_path)\n",
    "        self.bart_model = BartForConditionalGeneration.from_pretrained(bart_path).to(device)\n",
    "\n",
    "        # Load LED model\n",
    "        self.led_tokenizer = LEDTokenizer.from_pretrained(led_path)\n",
    "        self.led_model = LEDForConditionalGeneration.from_pretrained(led_path).to(device)\n",
    "\n",
    "        # Load InLegalBERT for reranking\n",
    "        self.rerank_tokenizer = AutoTokenizer.from_pretrained(reranker_name)\n",
    "        self.rerank_model = AutoModel.from_pretrained(reranker_name).to(device)\n",
    "\n",
    "    def generate_summary(self, model, tokenizer, text, max_words=500, min_words=400):\n",
    "        # Adjust input length depending on model type\n",
    "        if \"bart\" in model.config.model_type.lower():\n",
    "            max_input_len = 1024\n",
    "        elif \"led\" in model.config.model_type.lower():\n",
    "            max_input_len = 4096\n",
    "        else:\n",
    "            max_input_len = 1024  # default safe limit\n",
    "\n",
    "        max_tokens = int(max_words * 1.5)\n",
    "        min_tokens = int(min_words * 1.5)\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_input_len).to(device)\n",
    "\n",
    "        # LED requires global attention on first token\n",
    "        if \"led\" in model.config.model_type.lower():\n",
    "            global_attention_mask = torch.zeros_like(inputs[\"input_ids\"])\n",
    "            global_attention_mask[:, 0] = 1\n",
    "            summary_ids = model.generate(\n",
    "                **inputs,\n",
    "                global_attention_mask=global_attention_mask,\n",
    "                max_length=max_tokens,\n",
    "                min_length=min_tokens,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        else:\n",
    "            summary_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_tokens,\n",
    "                min_length=min_tokens,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.rerank_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.rerank_model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "    def rerank(self, judgment, summaries):\n",
    "        j_emb = self.get_embedding(judgment)\n",
    "        sims = [cosine_similarity(j_emb, self.get_embedding(s))[0][0] for s in summaries]\n",
    "        return sims\n",
    "\n",
    "    def summarize(self, judgment_text):\n",
    "        bart_sum = self.generate_summary(self.bart_model, self.bart_tokenizer, judgment_text)\n",
    "        led_sum = self.generate_summary(self.led_model, self.led_tokenizer, judgment_text)\n",
    "        sims = self.rerank(judgment_text, [bart_sum, led_sum])\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        return [bart_sum, led_sum][best_idx], sims\n",
    "\n",
    "# =========================================================\n",
    "# 4 Load Training Dataset and Split 90/10\n",
    "# =========================================================\n",
    "train_judg_file = \"train_judg.jsonl\"    # {\"ID\": \"...\", \"Judgment\": \"...\"}\n",
    "train_summ_file = \"train_ref_summ.jsonl\"  # {\"ID\": \"...\", \"Summary\": \"...\"}\n",
    "\n",
    "train_judgs = []\n",
    "with jsonlines.open(train_judg_file) as f_j, jsonlines.open(train_summ_file) as f_s:\n",
    "    for j_obj, s_obj in zip(f_j, f_s):\n",
    "        train_judgs.append({\n",
    "            \"ID\": j_obj[\"ID\"],\n",
    "            \"text\": clean_text(j_obj[\"Judgment\"]),\n",
    "            \"summary\": s_obj[\"Summary\"]\n",
    "        })\n",
    "\n",
    "train_list, val_list = train_test_split(train_judgs, test_size=0.1, random_state=42)\n",
    "print(f\" Dataset split: {len(train_list)} training, {len(val_list)} validation\")\n",
    "\n",
    "# =========================================================\n",
    "# 5 Initialize Ensemble\n",
    "# =========================================================\n",
    "bart_path = \"bart_legal_summ_model_final\"\n",
    "led_path = \"led_legal_summ_model_final\"\n",
    "\n",
    "ensemble = LegalEnsembleSummarizer(bart_path, led_path)\n",
    "\n",
    "# =========================================================\n",
    "# 6 Generate Summaries for Validation Set\n",
    "# =========================================================\n",
    "generated_summaries = []\n",
    "\n",
    "print(\"Generating summaries for validation set...\")\n",
    "for example in tqdm(val_list):\n",
    "    text = example[\"text\"][:8000]  # limit to 8000 characters for safety\n",
    "    summary, sims = ensemble.summarize(text)\n",
    "    generated_summaries.append({\n",
    "        \"ID\": example[\"ID\"],\n",
    "        \"generated_summary\": summary,\n",
    "        \"reference_summary\": example[\"summary\"],\n",
    "        \"bart_similarity\": float(sims[0]),\n",
    "        \"led_similarity\": float(sims[1]),\n",
    "        \"chosen_model\": \"BART\" if sims[0] > sims[1] else \"LED\"\n",
    "    })\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "val_output_file = \"outputs/val_generated_summaries.jsonl\"\n",
    "with jsonlines.open(val_output_file, mode=\"w\") as writer:\n",
    "    writer.write_all(generated_summaries)\n",
    "\n",
    "print(f\" Validation summaries saved to {val_output_file}\")\n",
    "\n",
    "# =========================================================\n",
    "# 7 Evaluation Metrics: ROUGE, BLEU, BERTScore\n",
    "# =========================================================\n",
    "rouge_scorer = Rouge()\n",
    "bleu_scorer = BLEU()\n",
    "bertscore_eval = evaluate.load(\"bertscore\")\n",
    "\n",
    "rouge1_scores, rouge2_scores, rougel_scores, bleu_scores, bert_scores = [], [], [], [], []\n",
    "\n",
    "for item in generated_summaries:\n",
    "    hyp = item[\"generated_summary\"]\n",
    "    ref = item[\"reference_summary\"]\n",
    "\n",
    "    # ROUGE\n",
    "    try:\n",
    "        r = rouge_scorer.get_scores(hyp, ref)[0]\n",
    "        rouge1 = r[\"rouge-1\"][\"f\"] * 100\n",
    "        rouge2 = r[\"rouge-2\"][\"f\"] * 100\n",
    "        rougel = r[\"rouge-l\"][\"f\"] * 100\n",
    "    except:\n",
    "        rouge1, rouge2, rougel = 0, 0, 0\n",
    "\n",
    "    # BLEU\n",
    "    try:\n",
    "        bleu = bleu_scorer.sentence_score(hyp, [ref]).score\n",
    "    except:\n",
    "        bleu = 0\n",
    "\n",
    "    # BERTScore\n",
    "    try:\n",
    "        bert = bertscore_eval.compute(predictions=[hyp], references=[ref], lang=\"en\")[\"f1\"][0] * 100\n",
    "    except:\n",
    "        bert = 0\n",
    "\n",
    "    rouge1_scores.append(rouge1)\n",
    "    rouge2_scores.append(rouge2)\n",
    "    rougel_scores.append(rougel)\n",
    "    bleu_scores.append(bleu)\n",
    "    bert_scores.append(bert)\n",
    "\n",
    "metrics = {\n",
    "    \"ROUGE-1 (%)\": float(np.mean(rouge1_scores)),\n",
    "    \"ROUGE-2 (%)\": float(np.mean(rouge2_scores)),\n",
    "    \"ROUGE-L (%)\": float(np.mean(rougel_scores)),\n",
    "    \"BLEU (%)\": float(np.mean(bleu_scores)),\n",
    "    \"BERTScore-F1 (%)\": float(np.mean(bert_scores)),\n",
    "    \"AVG_SCORE (%)\": float(np.mean([np.mean(rouge2_scores), np.mean(rougel_scores), np.mean(bleu_scores)]))\n",
    "}\n",
    "\n",
    "metrics_path = \"outputs/val_metrics.json\"\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(\"\\n========== ðŸ§¾ Validation Metrics ==========\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# =========================================================\n",
    "# 8 Save the Full Ensemble Folder\n",
    "# =========================================================\n",
    "save_dir = \"ensemble_legal_model\"\n",
    "\n",
    "def save_full_ensemble(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    shutil.copytree(bart_path, os.path.join(save_dir, \"bart_model\"), dirs_exist_ok=True)\n",
    "    shutil.copytree(led_path, os.path.join(save_dir, \"led_model\"), dirs_exist_ok=True)\n",
    "\n",
    "    metadata = {\n",
    "        \"bart_model\": \"bart_model\",\n",
    "        \"led_model\": \"led_model\",\n",
    "        \"reranker_model\": \"law-ai/InLegalBERT\",\n",
    "        \"description\": \"Ensemble of fine-tuned BART + LED reranked with InLegalBERT for legal summarization\"\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"ensemble_metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    print(f\" Full ensemble saved at: {save_dir}\")\n",
    "\n",
    "save_full_ensemble(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393a4b8-e62a-4fac-8a01-cd30ca2df39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
