{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#  LEGAL ENSEMBLE SUMMARIZER â€” Train & Evaluate on Validation Set (Fixed JSON float issue)\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import (\n",
    "    BartTokenizer, BartForConditionalGeneration,\n",
    "    PegasusTokenizer, PegasusForConditionalGeneration,\n",
    "    LEDTokenizer, LEDForConditionalGeneration\n",
    ")\n",
    "from rouge import Rouge\n",
    "from sacrebleu.metrics import BLEU\n",
    "import evaluate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# =========================================================\n",
    "# 1 Clean Judgment Text\n",
    "# =========================================================\n",
    "def clean_judgment_text(text):\n",
    "    text = re.sub(r\"\\[Page No\\.\\s*\\d+\\]\", \" \", text)\n",
    "    text = re.sub(r\"Case\\s*:-.*?\\n\", \" \", text)\n",
    "    text = re.sub(\n",
    "        r\"(Petitioner\\s*:-.*?Respondent\\s*:-.*?Counsel for Respondent\\s*:-.*?)(\\1)+\",\n",
    "        r\"\\1\",\n",
    "        text,\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "    text = re.sub(r\"\\(\\d+\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.replace(\" ,\", \",\").replace(\" .\", \".\").strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2 Ensemble Class\n",
    "# =========================================================\n",
    "class LegalEnsembleSummarizer:\n",
    "    def __init__(self, bart_path, pegasus_path, led_path):\n",
    "        self.bart_tokenizer = BartTokenizer.from_pretrained(bart_path)\n",
    "        self.bart_model = BartForConditionalGeneration.from_pretrained(bart_path).to(device)\n",
    "        self.bart_model.eval()\n",
    "\n",
    "        self.pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_path)\n",
    "        self.pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_path).to(device)\n",
    "        self.pegasus_model.eval()\n",
    "\n",
    "        self.led_tokenizer = LEDTokenizer.from_pretrained(led_path)\n",
    "        self.led_model = LEDForConditionalGeneration.from_pretrained(led_path).to(device)\n",
    "        self.led_model.eval()\n",
    "\n",
    "    def generate_summary(self, model, tokenizer, text, max_words=500, min_words=400, model_type=\"bart\"):\n",
    "        max_tokens = int(max_words * 1.5)\n",
    "        min_tokens = int(min_words * 1.5)\n",
    "        if model_type == \"led\":\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=16384).to(device)\n",
    "        else:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "        summary_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_tokens,\n",
    "            min_length=min_tokens,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.bart_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = self.bart_model.model.encoder(**inputs)\n",
    "        emb = encoder_outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        return emb\n",
    "\n",
    "    def rerank(self, judgment, summaries):\n",
    "        j_emb = self.get_embedding(judgment)\n",
    "        sims = [cosine_similarity(j_emb, self.get_embedding(s))[0][0] for s in summaries]\n",
    "        return sims\n",
    "\n",
    "    def summarize(self, judgment_text):\n",
    "        bart_sum = self.generate_summary(self.bart_model, self.bart_tokenizer, judgment_text, model_type=\"bart\")\n",
    "        peg_sum = self.generate_summary(self.pegasus_model, self.pegasus_tokenizer, judgment_text, model_type=\"pegasus\")\n",
    "        led_sum = self.generate_summary(self.led_model, self.led_tokenizer, judgment_text, model_type=\"led\")\n",
    "\n",
    "        summaries = [bart_sum, peg_sum, led_sum]\n",
    "        sims = self.rerank(judgment_text, summaries)\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        best_summary = summaries[best_idx]\n",
    "        return best_summary, sims\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3 Load Dataset & Split\n",
    "# =========================================================\n",
    "train_text_file = \"train_judg.jsonl\"\n",
    "train_summ_file = \"train_ref_summ.jsonl\"\n",
    "\n",
    "all_texts = [json.loads(line) for line in open(train_text_file, \"r\", encoding=\"utf-8\")]\n",
    "all_summaries = [json.loads(line) for line in open(train_summ_file, \"r\", encoding=\"utf-8\")]\n",
    "\n",
    "all_data = [{\"ID\": t[\"ID\"], \"text\": clean_judgment_text(t[\"Judgment\"]), \"summary\": s[\"Summary\"]}\n",
    "            for t, s in zip(all_texts, all_summaries)]\n",
    "\n",
    "train_list, val_list = train_test_split(all_data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\" Dataset split: {len(train_list)} training, {len(val_list)} validation\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4 Initialize Ensemble\n",
    "# =========================================================\n",
    "bart_path = \"bart_model\"\n",
    "pegasus_path = \"pegasus_model\"\n",
    "led_path = \"led_model\"\n",
    "\n",
    "ensemble = LegalEnsembleSummarizer(bart_path, pegasus_path, led_path)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5 Generate Summaries for Validation\n",
    "# =========================================================\n",
    "generated_summaries = []\n",
    "\n",
    "print(\"Generating ensemble summaries for validation set (400â€“500 words)...\")\n",
    "for example in tqdm(val_list):\n",
    "    text = example[\"text\"][:16384]  # LED supports long inputs\n",
    "    best_summary, sims = ensemble.summarize(text)\n",
    "    generated_summaries.append({\n",
    "        \"ID\": example[\"ID\"],\n",
    "        \"generated_summary\": best_summary,\n",
    "        \"reference_summary\": example[\"summary\"],\n",
    "        \"similarities\": [float(x) for x in sims]\n",
    "    })\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "val_output_file = \"outputs/ensemble_val_summaries.jsonl\"\n",
    "with open(val_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in generated_summaries:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\" Validation summaries saved to {val_output_file}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6 Evaluate Summaries\n",
    "# =========================================================\n",
    "rouge_scorer = Rouge()\n",
    "bleu_scorer = BLEU()\n",
    "bertscore_eval = evaluate.load(\"bertscore\")\n",
    "\n",
    "rouge1_scores, rouge2_scores, rougel_scores, bleu_scores, bert_scores = [], [], [], [], []\n",
    "\n",
    "for item in generated_summaries:\n",
    "    hyp = item[\"generated_summary\"]\n",
    "    ref = item[\"reference_summary\"]\n",
    "\n",
    "    try:\n",
    "        r = rouge_scorer.get_scores(hyp, ref)[0]\n",
    "        rouge1 = r[\"rouge-1\"][\"f\"] * 100\n",
    "        rouge2 = r[\"rouge-2\"][\"f\"] * 100\n",
    "        rougel = r[\"rouge-l\"][\"f\"] * 100\n",
    "    except Exception:\n",
    "        rouge1, rouge2, rougel = 0, 0, 0\n",
    "\n",
    "    try:\n",
    "        bleu = bleu_scorer.sentence_score(hyp, [ref]).score\n",
    "    except Exception:\n",
    "        bleu = 0\n",
    "\n",
    "    try:\n",
    "        bert = bertscore_eval.compute(predictions=[hyp], references=[ref], lang=\"en\")[\"f1\"][0] * 100\n",
    "    except Exception:\n",
    "        bert = 0\n",
    "\n",
    "    rouge1_scores.append(rouge1)\n",
    "    rouge2_scores.append(rouge2)\n",
    "    rougel_scores.append(rougel)\n",
    "    bleu_scores.append(bleu)\n",
    "    bert_scores.append(bert)\n",
    "\n",
    "metrics = {\n",
    "    \"ROUGE-1 (%)\": np.mean(rouge1_scores),\n",
    "    \"ROUGE-2 (%)\": np.mean(rouge2_scores),\n",
    "    \"ROUGE-L (%)\": np.mean(rougel_scores),\n",
    "    \"BLEU (%)\": np.mean(bleu_scores),\n",
    "    \"BERTScore-F1 (%)\": np.mean(bert_scores),\n",
    "    \"AVG_SCORE (%)\": np.mean([np.mean(rouge2_scores), np.mean(rougel_scores), np.mean(bleu_scores)])\n",
    "}\n",
    "\n",
    "#  Convert NumPy floats to Python floats before saving\n",
    "metrics = {k: float(v) if isinstance(v, (np.float32, np.float64, np.float_)) else v for k, v in metrics.items()}\n",
    "\n",
    "metrics_path = \"outputs/ensemble_val_metrics.json\"\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(\"\\n========== ðŸ§¾ Ensemble Validation Metrics (in %) ==========\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "print(\"===========================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b98dc-277b-444e-8fba-1293aeb702f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#  Legal Ensemble Summarizer â€” BART + Pegasus + LED\n",
    "#  Generates summaries (400â€“500 words), reranks using fine-tuned BART embeddings,\n",
    "#    saves outputs in JSONL format:\n",
    "#    {\"ID\": \"<datapoint-id>\", \"Summary\": \"<generated summary>\"}\n",
    "# =========================================================\n",
    "\n",
    "import torch, re, os, json, jsonlines, shutil\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BartTokenizer, BartForConditionalGeneration,\n",
    "    PegasusTokenizer, PegasusForConditionalGeneration,\n",
    "    LEDTokenizer, LEDForConditionalGeneration\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# =========================================================\n",
    "# 1 Device setup\n",
    "# =========================================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# =========================================================\n",
    "# 2 Legal-specific preprocessing function\n",
    "# =========================================================\n",
    "def clean_judgment_text(text):\n",
    "    text = re.sub(r\"\\[Page No\\.\\s*\\d+\\]\", \" \", text)\n",
    "    text = re.sub(r\"Case\\s*:-.*?\\n\", \" \", text)\n",
    "    text = re.sub(\n",
    "        r\"(Petitioner\\s*:-.*?Respondent\\s*:-.*?Counsel for Respondent\\s*:-.*?)(\\1)+\",\n",
    "        r\"\\1\",\n",
    "        text,\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "    text = re.sub(r\"\\(\\d+\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.replace(\" ,\", \",\").replace(\" .\", \".\").strip()\n",
    "    return text\n",
    "\n",
    "# =========================================================\n",
    "# 3 Ensemble Summarizer Class\n",
    "# =========================================================\n",
    "class LegalEnsembleSummarizer:\n",
    "    def __init__(self, bart_path, pegasus_path, led_path):\n",
    "        # Load BART model (for generation & embedding)\n",
    "        self.bart_tokenizer = BartTokenizer.from_pretrained(bart_path)\n",
    "        self.bart_model = BartForConditionalGeneration.from_pretrained(bart_path).to(device)\n",
    "        self.bart_model.eval()  # generator & reranker\n",
    "\n",
    "        # Load Pegasus model\n",
    "        self.pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_path)\n",
    "        self.pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_path).to(device)\n",
    "        self.pegasus_model.eval()\n",
    "\n",
    "        # Load LED model\n",
    "        self.led_tokenizer = LEDTokenizer.from_pretrained(led_path)\n",
    "        self.led_model = LEDForConditionalGeneration.from_pretrained(led_path).to(device)\n",
    "        self.led_model.eval()\n",
    "\n",
    "    # Generate summary from a model\n",
    "    def generate_summary(self, model, tokenizer, text, max_words=500, min_words=400, model_type=\"bart\"):\n",
    "        max_tokens = int(max_words * 1.5)\n",
    "        min_tokens = int(min_words * 1.5)\n",
    "        if model_type == \"led\":\n",
    "            # LED supports long documents\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=16384).to(device)\n",
    "        else:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "        summary_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_tokens,\n",
    "            min_length=min_tokens,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Get embeddings using BART encoder\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.bart_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = self.bart_model.model.encoder(**inputs)\n",
    "        emb = encoder_outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        return emb\n",
    "\n",
    "    # Compute similarity of each summary with judgment\n",
    "    def rerank(self, judgment, summaries):\n",
    "        j_emb = self.get_embedding(judgment)\n",
    "        sims = [cosine_similarity(j_emb, self.get_embedding(s))[0][0] for s in summaries]\n",
    "        return sims\n",
    "\n",
    "    # Generate summaries and select best one\n",
    "    def summarize(self, judgment_text):\n",
    "        bart_sum = self.generate_summary(self.bart_model, self.bart_tokenizer, judgment_text, model_type=\"bart\")\n",
    "        peg_sum = self.generate_summary(self.pegasus_model, self.pegasus_tokenizer, judgment_text, model_type=\"pegasus\")\n",
    "        led_sum = self.generate_summary(self.led_model, self.led_tokenizer, judgment_text, model_type=\"led\")\n",
    "\n",
    "        summaries = [bart_sum, peg_sum, led_sum]\n",
    "        sims = self.rerank(judgment_text, summaries)\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        best_summary = summaries[best_idx]\n",
    "        return best_summary, sims\n",
    "\n",
    "# =========================================================\n",
    "# 4 Initialize paths & models\n",
    "# =========================================================\n",
    "bart_path = \"bart_legal_summ_model_final\"\n",
    "pegasus_path = \"pegasus_legal_summ_model_final\"\n",
    "led_path = \"led_legal_summ_model_final\"\n",
    "test_path = \"test_judg.jsonl\"       # Input JSONL file: {\"id\": \"<ID>\", \"judgment\": \"<text>\"}\n",
    "save_dir = \"ensemble_legal_model\"\n",
    "output_file = \"generated_summaries.jsonl\"\n",
    "similarity_report_file = \"ensemble_test_similarity_report.jsonl\"\n",
    "\n",
    "ensemble = LegalEnsembleSummarizer(bart_path, pegasus_path, led_path)\n",
    "\n",
    "# =========================================================\n",
    "# 5 Load Test Data\n",
    "# =========================================================\n",
    "judgments = []\n",
    "with jsonlines.open(test_path) as reader:\n",
    "    for obj in reader:\n",
    "        judgments.append({\n",
    "            \"ID\": obj[\"id\"],\n",
    "            \"text\": clean_judgment_text(obj[\"judgment\"])\n",
    "        })\n",
    "\n",
    "print(f\"Loaded {len(judgments)} test cases\")\n",
    "\n",
    "# =========================================================\n",
    "# 6 Generate Summaries & Compute Cosine Similarities\n",
    "# =========================================================\n",
    "similarity_report = []\n",
    "\n",
    "with jsonlines.open(output_file, mode=\"w\") as writer:\n",
    "    for idx, j in enumerate(tqdm(judgments, desc=\"Generating ensemble summaries\")):\n",
    "        text = j[\"text\"][:16384]  # LED can handle long inputs\n",
    "        summary, sims = ensemble.summarize(text)\n",
    "        best_sim = float(max(sims))\n",
    "        chosen_model = [\"BART\", \"Pegasus\", \"LED\"][int(np.argmax(sims))]\n",
    "\n",
    "        # Save summary\n",
    "        writer.write({\n",
    "            \"ID\": j[\"ID\"],\n",
    "            \"Summary\": summary\n",
    "        })\n",
    "\n",
    "        # Save similarity info\n",
    "        similarity_report.append({\n",
    "            \"ID\": j[\"ID\"],\n",
    "            \"bart_similarity\": float(sims[0]),\n",
    "            \"pegasus_similarity\": float(sims[1]),\n",
    "            \"led_similarity\": float(sims[2]),\n",
    "            \"chosen_model\": chosen_model,\n",
    "            \"best_similarity\": best_sim\n",
    "        })\n",
    "\n",
    "        if idx < 10:\n",
    "            print(f\"\\nID: {j['ID']}\")\n",
    "            print(f\"   BART sim: {sims[0]:.4f}\")\n",
    "            print(f\"   Pegasus sim: {sims[1]:.4f}\")\n",
    "            print(f\"   LED sim: {sims[2]:.4f}\")\n",
    "            print(f\"   Chosen: {chosen_model} (Cosine={best_sim:.4f})\")\n",
    "\n",
    "# =========================================================\n",
    "# 7 Save Similarity Report\n",
    "# =========================================================\n",
    "with jsonlines.open(similarity_report_file, mode=\"w\") as writer:\n",
    "    writer.write_all(similarity_report)\n",
    "\n",
    "print(\"\\n All summaries generated successfully!\")\n",
    "print(f\"  Saved summaries â†’ {output_file}\")\n",
    "print(f\"  Saved similarity report â†’ {similarity_report_file}\")\n",
    "\n",
    "# =========================================================\n",
    "# 8 Save Full Ensemble Folder\n",
    "# =========================================================\n",
    "def save_full_ensemble(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    shutil.copytree(bart_path, os.path.join(save_dir, \"bart_model\"), dirs_exist_ok=True)\n",
    "    shutil.copytree(pegasus_path, os.path.join(save_dir, \"pegasus_model\"), dirs_exist_ok=True)\n",
    "    shutil.copytree(led_path, os.path.join(save_dir, \"led_model\"), dirs_exist_ok=True)\n",
    "    \n",
    "    metadata = {\n",
    "        \"bart_model\": \"bart_model\",\n",
    "        \"pegasus_model\": \"pegasus_model\",\n",
    "        \"led_model\": \"led_model\",\n",
    "        \"reranker_model\": \"BART (fine-tuned)\",\n",
    "        \"description\": \"Ensemble of fine-tuned BART + legal Pegasus + LED reranked using BART embeddings\"\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"ensemble_metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\" Full ensemble saved at: {save_dir}\")\n",
    "\n",
    "save_full_ensemble(save_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
