{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#  Legal Ensemble Summarizer â€” BART + Pegasus + InLegalBERT\n",
    "#  Generates summaries (400â€“500 words) for validation set,\n",
    "#    reranks using InLegalBERT, and calculates evaluation scores\n",
    "# =========================================================\n",
    "\n",
    "import torch, re, os, json, jsonlines, shutil\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BartTokenizer, BartForConditionalGeneration,\n",
    "    PegasusTokenizer, PegasusForConditionalGeneration,\n",
    "    AutoTokenizer, AutoModel\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from sacrebleu.metrics import BLEU\n",
    "import evaluate\n",
    "\n",
    "# =========================================================\n",
    "# 1 Setup\n",
    "# =========================================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# =========================================================\n",
    "# 2 Preprocessing\n",
    "# =========================================================\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'Page\\s*\\d+\\s*of\\s*\\d+', ' ', text)\n",
    "    text = re.sub(r'(Case\\s*No\\.?|Crl\\.A\\.No\\.?|Appeal\\s*No\\.?)\\s*[\\w/-]+', ' ', text)\n",
    "    text = re.sub(r'\\(\\d{4}\\)\\s*\\d+\\s*[A-Z]+\\s*\\d+', ' ', text)\n",
    "    text = re.sub(r'AIR\\s*\\d{4}\\s*[A-Z]+\\s*\\d+', ' ', text)\n",
    "    text = re.sub(r'Dated\\s*[:\\-]?\\s*\\d{1,2}[-./]\\d{1,2}[-./]\\d{2,4}', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# =========================================================\n",
    "# 3 Ensemble Summarizer Class\n",
    "# =========================================================\n",
    "class LegalEnsembleSummarizer:\n",
    "    def __init__(self, bart_path, pegasus_path, reranker_name=\"law-ai/InLegalBERT\"):\n",
    "        # Load BART model\n",
    "        self.bart_tokenizer = BartTokenizer.from_pretrained(bart_path)\n",
    "        self.bart_model = BartForConditionalGeneration.from_pretrained(bart_path).to(device)\n",
    "\n",
    "        # Load Pegasus model\n",
    "        self.pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_path)\n",
    "        self.pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_path).to(device)\n",
    "\n",
    "        # Load InLegalBERT for reranking\n",
    "        self.rerank_tokenizer = AutoTokenizer.from_pretrained(reranker_name)\n",
    "        self.rerank_model = AutoModel.from_pretrained(reranker_name).to(device)\n",
    "\n",
    "    def generate_summary(self, model, tokenizer, text, max_words=500, min_words=400):\n",
    "        max_tokens = int(max_words * 1.5)\n",
    "        min_tokens = int(min_words * 1.5)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "        summary_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_tokens,\n",
    "            min_length=min_tokens,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.rerank_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.rerank_model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "    def rerank(self, judgment, summaries):\n",
    "        j_emb = self.get_embedding(judgment)\n",
    "        sims = [cosine_similarity(j_emb, self.get_embedding(s))[0][0] for s in summaries]\n",
    "        return sims\n",
    "\n",
    "    def summarize(self, judgment_text):\n",
    "        bart_sum = self.generate_summary(self.bart_model, self.bart_tokenizer, judgment_text)\n",
    "        peg_sum = self.generate_summary(self.pegasus_model, self.pegasus_tokenizer, judgment_text)\n",
    "        sims = self.rerank(judgment_text, [bart_sum, peg_sum])\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        return [bart_sum, peg_sum][best_idx], sims\n",
    "\n",
    "# =========================================================\n",
    "# 4 Load Training Dataset and Split 90/10\n",
    "# =========================================================\n",
    "train_judg_file = \"train_judg.jsonl\"    # {\"ID\": \"...\", \"Judgment\": \"...\"}\n",
    "train_summ_file = \"train_ref_summ.jsonl\"  # {\"ID\": \"...\", \"Summary\": \"...\"}\n",
    "\n",
    "train_judgs = []\n",
    "with jsonlines.open(train_judg_file) as f_j, jsonlines.open(train_summ_file) as f_s:\n",
    "    for j_obj, s_obj in zip(f_j, f_s):\n",
    "        train_judgs.append({\n",
    "            \"ID\": j_obj[\"ID\"],\n",
    "            \"text\": clean_text(j_obj[\"Judgment\"]),\n",
    "            \"summary\": s_obj[\"Summary\"]\n",
    "        })\n",
    "\n",
    "train_list, val_list = train_test_split(train_judgs, test_size=0.1, random_state=42)\n",
    "print(f\" Dataset split: {len(train_list)} training, {len(val_list)} validation\")\n",
    "\n",
    "# =========================================================\n",
    "# 5 Initialize Ensemble\n",
    "# =========================================================\n",
    "bart_path = \"bart_legal_summ_model_final\"\n",
    "pegasus_path = \"pegasus_legal_summ_model_final\"\n",
    "\n",
    "ensemble = LegalEnsembleSummarizer(bart_path, pegasus_path)\n",
    "\n",
    "# =========================================================\n",
    "# 6 Generate Summaries for Validation Set\n",
    "# =========================================================\n",
    "generated_summaries = []\n",
    "\n",
    "print(\"Generating summaries for validation set...\")\n",
    "for example in tqdm(val_list):\n",
    "    text = example[\"text\"][:4096]  # limit to 4096 tokens\n",
    "    summary, sims = ensemble.summarize(text)\n",
    "    generated_summaries.append({\n",
    "        \"ID\": example[\"ID\"],\n",
    "        \"generated_summary\": summary,\n",
    "        \"reference_summary\": example[\"summary\"],\n",
    "        \"bart_similarity\": float(sims[0]),\n",
    "        \"pegasus_similarity\": float(sims[1]),\n",
    "        \"chosen_model\": \"BART\" if sims[0] > sims[1] else \"Pegasus\"\n",
    "    })\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "val_output_file = \"outputs/val_generated_summaries.jsonl\"\n",
    "with jsonlines.open(val_output_file, mode=\"w\") as writer:\n",
    "    writer.write_all(generated_summaries)\n",
    "\n",
    "print(f\" Validation summaries saved to {val_output_file}\")\n",
    "\n",
    "# =========================================================\n",
    "# 7 Evaluation Metrics: ROUGE, BLEU, BERTScore\n",
    "# =========================================================\n",
    "rouge_scorer = Rouge()\n",
    "bleu_scorer = BLEU()\n",
    "bertscore_eval = evaluate.load(\"bertscore\")\n",
    "\n",
    "rouge1_scores, rouge2_scores, rougel_scores, bleu_scores, bert_scores = [], [], [], [], []\n",
    "\n",
    "for item in generated_summaries:\n",
    "    hyp = item[\"generated_summary\"]\n",
    "    ref = item[\"reference_summary\"]\n",
    "\n",
    "    # ROUGE\n",
    "    try:\n",
    "        r = rouge_scorer.get_scores(hyp, ref)[0]\n",
    "        rouge1 = r[\"rouge-1\"][\"f\"] * 100\n",
    "        rouge2 = r[\"rouge-2\"][\"f\"] * 100\n",
    "        rougel = r[\"rouge-l\"][\"f\"] * 100\n",
    "    except:\n",
    "        rouge1, rouge2, rougel = 0, 0, 0\n",
    "\n",
    "    # BLEU\n",
    "    try:\n",
    "        bleu = bleu_scorer.sentence_score(hyp, [ref]).score\n",
    "    except:\n",
    "        bleu = 0\n",
    "\n",
    "    # BERTScore\n",
    "    try:\n",
    "        bert = bertscore_eval.compute(predictions=[hyp], references=[ref], lang=\"en\")[\"f1\"][0] * 100\n",
    "    except:\n",
    "        bert = 0\n",
    "\n",
    "    rouge1_scores.append(rouge1)\n",
    "    rouge2_scores.append(rouge2)\n",
    "    rougel_scores.append(rougel)\n",
    "    bleu_scores.append(bleu)\n",
    "    bert_scores.append(bert)\n",
    "\n",
    "metrics = {\n",
    "    \"ROUGE-1 (%)\": float(np.mean(rouge1_scores)),\n",
    "    \"ROUGE-2 (%)\": float(np.mean(rouge2_scores)),\n",
    "    \"ROUGE-L (%)\": float(np.mean(rougel_scores)),\n",
    "    \"BLEU (%)\": float(np.mean(bleu_scores)),\n",
    "    \"BERTScore-F1 (%)\": float(np.mean(bert_scores)),\n",
    "    \"AVG_SCORE (%)\": float(np.mean([np.mean(rouge2_scores), np.mean(rougel_scores), np.mean(bleu_scores)]))\n",
    "}\n",
    "\n",
    "metrics_path = \"outputs/val_metrics.json\"\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(\"\\n========== Validation Metrics ==========\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# =========================================================\n",
    "# 8 Save the Full Ensemble Folder\n",
    "# =========================================================\n",
    "save_dir = \"ensemble_legal_model\"\n",
    "def save_full_ensemble(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    shutil.copytree(bart_path, os.path.join(save_dir, \"bart_model\"), dirs_exist_ok=True)\n",
    "    shutil.copytree(pegasus_path, os.path.join(save_dir, \"pegasus_model\"), dirs_exist_ok=True)\n",
    "    \n",
    "    metadata = {\n",
    "        \"bart_model\": \"bart_model\",\n",
    "        \"pegasus_model\": \"pegasus_model\",\n",
    "        \"reranker_model\": \"law-ai/InLegalBERT\",\n",
    "        \"description\": \"Ensemble of fine-tuned BART + legal Pegasus reranked with InLegalBERT for legal summarization\"\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"ensemble_metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Full ensemble saved at: {save_dir}\")\n",
    "\n",
    "save_full_ensemble(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199c589-b0d1-4728-8449-f4196645bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#  Legal Ensemble Summarizer â€” BART + Pegasus + InLegalBERT\n",
    "#  Generates summaries (400â€“500 words), reranks using InLegalBERT,\n",
    "#    saves outputs in JSONL format:\n",
    "#    {\"ID\": \"<datapoint-id>\", \"Summary\": \"<generated summary by your model>\"}\n",
    "# =========================================================\n",
    "\n",
    "import torch, re, os, json, jsonlines, shutil\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BartTokenizer, BartForConditionalGeneration,\n",
    "    PegasusTokenizer, PegasusForConditionalGeneration,\n",
    "    AutoTokenizer, AutoModel\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# =========================================================\n",
    "# 1 Setup\n",
    "# =========================================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# =========================================================\n",
    "# 2 Preprocessing\n",
    "# =========================================================\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'Page\\s*\\d+\\s*of\\s*\\d+', ' ', text)\n",
    "    text = re.sub(r'(Case\\s*No\\.?|Crl\\.A\\.No\\.?|Appeal\\s*No\\.?)\\s*[\\w/-]+', ' ', text)\n",
    "    text = re.sub(r'\\(\\d{4}\\)\\s*\\d+\\s*[A-Z]+\\s*\\d+', ' ', text)\n",
    "    text = re.sub(r'AIR\\s*\\d{4}\\s*[A-Z]+\\s*\\d+', ' ', text)\n",
    "    text = re.sub(r'Dated\\s*[:\\-]?\\s*\\d{1,2}[-./]\\d{1,2}[-./]\\d{2,4}', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# =========================================================\n",
    "# 3 Ensemble Summarizer Class\n",
    "# =========================================================\n",
    "class LegalEnsembleSummarizer:\n",
    "    def __init__(self, bart_path, pegasus_path, reranker_name=\"law-ai/InLegalBERT\"):\n",
    "        # Load BART model\n",
    "        self.bart_tokenizer = BartTokenizer.from_pretrained(bart_path)\n",
    "        self.bart_model = BartForConditionalGeneration.from_pretrained(bart_path).to(device)\n",
    "\n",
    "        # Load Pegasus model\n",
    "        self.pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_path)\n",
    "        self.pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_path).to(device)\n",
    "\n",
    "        # Load InLegalBERT for reranking\n",
    "        self.rerank_tokenizer = AutoTokenizer.from_pretrained(reranker_name)\n",
    "        self.rerank_model = AutoModel.from_pretrained(reranker_name).to(device)\n",
    "\n",
    "    def generate_summary(self, model, tokenizer, text, max_words=500, min_words=400):\n",
    "        max_tokens = int(max_words * 1.5)\n",
    "        min_tokens = int(min_words * 1.5)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "        summary_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_tokens,\n",
    "            min_length=min_tokens,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.rerank_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.rerank_model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "    def rerank(self, judgment, summaries):\n",
    "        j_emb = self.get_embedding(judgment)\n",
    "        sims = [cosine_similarity(j_emb, self.get_embedding(s))[0][0] for s in summaries]\n",
    "        return sims\n",
    "\n",
    "    def summarize(self, judgment_text):\n",
    "        bart_sum = self.generate_summary(self.bart_model, self.bart_tokenizer, judgment_text)\n",
    "        peg_sum = self.generate_summary(self.pegasus_model, self.pegasus_tokenizer, judgment_text)\n",
    "        sims = self.rerank(judgment_text, [bart_sum, peg_sum])\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        return [bart_sum, peg_sum][best_idx], sims\n",
    "\n",
    "# =========================================================\n",
    "# 4 Initialize Models & Paths\n",
    "# =========================================================\n",
    "bart_path = \"bart_legal_summ_model_final\"\n",
    "pegasus_path = \"pegasus_legal_summ_model_final\"\n",
    "test_path = \"test_judg.jsonl\"     # Contains \"id\" and \"judgment\"\n",
    "save_dir = \"ensemble_legal_model\"\n",
    "\n",
    "ensemble = LegalEnsembleSummarizer(bart_path, pegasus_path)\n",
    "\n",
    "# =========================================================\n",
    "# 5 Load Test Data\n",
    "# =========================================================\n",
    "judgments = []\n",
    "with jsonlines.open(test_path) as reader:\n",
    "    for obj in reader:\n",
    "        judgments.append({\n",
    "            \"ID\": obj[\"id\"],             #  Updated key\n",
    "            \"text\": clean_text(obj[\"judgment\"])  #  Updated key\n",
    "        })\n",
    "judgments = judgments[:400]  # process first 200\n",
    "print(f\"Loaded {len(judgments)} test cases\")\n",
    "\n",
    "# =========================================================\n",
    "# 6 Generate Summaries + Cosine Similarities\n",
    "# =========================================================\n",
    "output_file = \"generated_summaries.jsonl\"\n",
    "similarity_report = []\n",
    "\n",
    "with jsonlines.open(output_file, mode=\"w\") as writer:\n",
    "    for idx, j in enumerate(tqdm(judgments, desc=\"Generating ensemble summaries\")):\n",
    "        text = j[\"text\"][:4096]  # limit length\n",
    "        summary, sims = ensemble.summarize(text)\n",
    "        best_sim = float(max(sims))\n",
    "\n",
    "        #  Write single-line JSON per summary\n",
    "        writer.write({\n",
    "            \"ID\": j[\"ID\"],\n",
    "            \"Summary\": summary\n",
    "        })\n",
    "\n",
    "        # Save similarity info (optional)\n",
    "        similarity_report.append({\n",
    "            \"ID\": j[\"ID\"],\n",
    "            \"bart_similarity\": float(sims[0]),\n",
    "            \"pegasus_similarity\": float(sims[1]),\n",
    "            \"chosen_model\": \"BART\" if sims[0] > sims[1] else \"Pegasus\",\n",
    "            \"best_similarity\": best_sim\n",
    "        })\n",
    "\n",
    "        if idx < 10:\n",
    "            print(f\"\\n ID: {j['ID']}\")\n",
    "            print(f\"   BART sim: {sims[0]:.4f}\")\n",
    "            print(f\"   Pegasus sim: {sims[1]:.4f}\")\n",
    "            print(f\"    Chosen: {'BART' if sims[0]>sims[1] else 'Pegasus'} (Cosine={best_sim:.4f})\")\n",
    "\n",
    "# =========================================================\n",
    "# 7 Save Similarity Report\n",
    "# =========================================================\n",
    "with jsonlines.open(\"ensemble_test_similarity_report.jsonl\", mode=\"w\") as writer:\n",
    "    writer.write_all(similarity_report)\n",
    "\n",
    "print(\"\\n All summaries generated successfully!\")\n",
    "print(\"  Saved summaries â†’ generated_summaries.jsonl\")\n",
    "print(\"  Saved similarity report â†’ ensemble_test_similarity_report.jsonl\")\n",
    "\n",
    "# =========================================================\n",
    "# 8 Save the Full Ensemble Folder\n",
    "# =========================================================\n",
    "def save_full_ensemble(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    shutil.copytree(bart_path, os.path.join(save_dir, \"bart_model\"), dirs_exist_ok=True)\n",
    "    shutil.copytree(pegasus_path, os.path.join(save_dir, \"pegasus_model\"), dirs_exist_ok=True)\n",
    "    \n",
    "    metadata = {\n",
    "        \"bart_model\": \"bart_model\",\n",
    "        \"pegasus_model\": \"pegasus_model\",\n",
    "        \"reranker_model\": \"law-ai/InLegalBERT\",\n",
    "        \"description\": \"Ensemble of fine-tuned BART + legal Pegasus reranked with InLegalBERT for legal summarization\"\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"ensemble_metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\" Full ensemble saved at: {save_dir}\")\n",
    "\n",
    "save_full_ensemble(save_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
